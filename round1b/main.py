import json
import datetime
import os
import PyPDF2
import pdfplumber
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import warnings
import time
from collections import Counter
import statistics
import nltk

# Download required NLTK data (one-time)
try:
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('corpora/stopwords')
    nltk.data.find('taggers/averaged_perceptron_tagger')
except LookupError:
    nltk.download('punkt')
    nltk.download('stopwords')
    nltk.download('averaged_perceptron_tagger')

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk import pos_tag

warnings.filterwarnings("ignore")

COLLECTIONS_BASE_PATH = "."

class PureMLAnalyzer:
    def __init__(self):
        print("ü§ñ Loading Pure ML Intelligence Analyzer (ZERO Hardcoding)...")
        start_time = time.time()
        try:
            self.model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')
            self.stop_words = set(stopwords.words('english'))
            
            load_time = time.time() - start_time
            print(f"‚úÖ Pure ML intelligence loaded in {load_time:.2f}s")
            print("üß† Using: Pure Semantic Understanding - Automatically detects ALL contexts")
            
        except Exception as e:
            print(f"‚ùå Error loading system: {e}")
            raise e
    
    def extract_text_with_formatting(self, pdf_path):
        """Extract text with formatting"""
        pages_data = {}
        
        try:
            with pdfplumber.open(pdf_path) as pdf:
                max_pages = min(50, len(pdf.pages))
                
                for page_num in range(max_pages):
                    page = pdf.pages[page_num]
                    chars = page.chars
                    if not chars:
                        continue
                    
                    lines_data = self.group_chars_into_lines(chars)
                    
                    if lines_data:
                        pages_data[page_num + 1] = {
                            'lines': lines_data,
                            'full_text': page.extract_text()
                        }
                        
        except Exception as e:
            return self.load_pdf_content_fallback(pdf_path)
        
        return {"pages": pages_data}
    
    def group_chars_into_lines(self, chars):
        """Group characters into lines with statistical features"""
        if not chars:
            return []
        
        lines = {}
        for char in chars:
            y_pos = round(char.get('y0', 0))
            if y_pos not in lines:
                lines[y_pos] = []
            lines[y_pos].append(char)
        
        lines_data = []
        
        for y_pos in sorted(lines.keys(), reverse=True):
            line_chars = sorted(lines[y_pos], key=lambda c: c.get('x0', 0))
            
            if not line_chars:
                continue
            
            line_text = ''.join([char.get('text', '') for char in line_chars]).strip()
            
            if len(line_text) < 2:
                continue
            
            # Extract statistical features
            font_sizes = [char.get('size', 12) for char in line_chars if char.get('size')]
            fonts = [char.get('fontname', '') for char in line_chars if char.get('fontname')]
            
            avg_font_size = statistics.mean(font_sizes) if font_sizes else 12
            max_font_size = max(font_sizes) if font_sizes else 12
            most_common_font = Counter(fonts).most_common(1)[0][0] if fonts else ''
            
            is_bold = any('bold' in font.lower() for font in fonts)
            
            lines_data.append({
                'text': line_text,
                'avg_font_size': avg_font_size,
                'max_font_size': max_font_size,
                'font_name': most_common_font,
                'is_bold': is_bold,
                'char_count': len(line_text),
                'word_count': len(line_text.split()),
                'y_position': y_pos
            })
        
        return lines_data
    
    def load_pdf_content_fallback(self, pdf_path):
        """Fallback PDF loading"""
        pages_content = {}
        try:
            with open(pdf_path, 'rb') as file:
                reader = PyPDF2.PdfReader(file)
                max_pages = min(50, len(reader.pages))
                
                for page_num in range(max_pages):
                    page = reader.pages[page_num]
                    text = page.extract_text()
                    if text and len(text.strip()) > 50:
                        lines = [line.strip() for line in text.split('\n') if line.strip()]
                        lines_data = []
                        
                        for i, line in enumerate(lines):
                            lines_data.append({
                                'text': line,
                                'avg_font_size': 12,
                                'max_font_size': 12,
                                'font_name': 'default',
                                'is_bold': False,
                                'char_count': len(line),
                                'word_count': len(line.split()),
                                'y_position': len(lines) - i
                            })
                        
                        pages_content[page_num + 1] = {
                            'lines': lines_data,
                            'full_text': text
                        }
        except Exception as e:
            print(f"‚ùå Error in fallback PDF reading: {e}")
        
        return {"pages": pages_content}
    
    def extract_best_heading_ml(self, page_data):
        """Extract heading using pure ML statistical analysis"""
        if 'lines' not in page_data:
            text = page_data.get('full_text', '')
            lines = [line.strip() for line in text.split('\n') if line.strip()]
            return lines[0] if lines else "Content"
        
        lines_data = page_data['lines']
        
        # Calculate document statistics
        font_sizes = [line['avg_font_size'] for line in lines_data]
        if not font_sizes:
            return "Content"
        
        max_font = max(font_sizes)
        mean_font = statistics.mean(font_sizes)
        q75_font = np.percentile(font_sizes, 75)
        
        # Find best heading candidates using pure statistics
        best_candidates = []
        
        for i, line in enumerate(lines_data[:15]):  # Check first 15 lines
            text = line['text'].strip()
            
            # Calculate statistical heading score
            score = 0
            
            # Font size analysis
            if line['avg_font_size'] == max_font:
                score += 5  # Highest font
            elif line['avg_font_size'] >= q75_font:
                score += 3  # Top 25% font
            elif line['avg_font_size'] > mean_font:
                score += 1  # Above average font
            
            # Bold text
            if line['is_bold']:
                score += 2
            
            # Length analysis - substantial titles
            if 15 <= line['char_count'] <= 120:
                score += 2
            elif 8 <= line['char_count'] <= 150:
                score += 1
            
            # Word count - reasonable for titles
            if 2 <= line['word_count'] <= 12:
                score += 1
            
            # Early position
            if i < 5:
                score += 1
            
            # Statistical quality indicators
            digit_ratio = sum(1 for c in text if c.isdigit()) / len(text) if text else 0
            if digit_ratio < 0.2:  # Low digit content
                score += 1
            
            if not text.lower().endswith(':'):  # Not a label
                score += 1
            
            if score > 4:
                best_candidates.append({
                    'text': text,
                    'score': score,
                    'index': i
                })
        
        # Return best candidate
        if best_candidates:
            best_candidates.sort(key=lambda x: (-x['score'], x['index']))
            return best_candidates[0]['text']
        
        # Fallback
        for line in lines_data[:10]:
            text = line['text'].strip()
            if (len(text) > 8 and 
                len(text) < 100 and
                line['word_count'] >= 2):
                return text
        
        return "Content"
    
    def generate_ml_queries(self, persona, job_description):
        """Generate queries using pure ML/NLP analysis"""
        try:
            combined_text = f"{persona}. {job_description}"
            
            # Primary query - most important
            queries = [f"{persona} {job_description}"]
            
            # Use NLP to extract key concepts
            tokens = word_tokenize(combined_text.lower())
            pos_tags = pos_tag(tokens)
            
            # Extract important concepts using linguistic analysis
            important_concepts = []
            for word, pos in pos_tags:
                if (pos.startswith(('NN', 'JJ', 'VB')) and 
                    len(word) > 3 and 
                    word not in self.stop_words):
                    important_concepts.append(word)
            
            # Create concept-based queries
            for i in range(0, min(len(important_concepts), 6), 2):
                if i + 1 < len(important_concepts):
                    query = f"{important_concepts[i]} {important_concepts[i+1]} information"
                    queries.append(query)
            
            # Individual important terms
            for concept in important_concepts[:4]:
                queries.append(f"{concept} related information details")
            
            return queries[:10]  # Limit for performance
            
        except Exception as e:
            print(f"Error generating ML queries: {e}")
            return [f"{persona} {job_description}"]
    
    def calculate_pure_semantic_relevance(self, page_content, queries, persona, job_description):
        """Pure ML intelligence - automatically understands ALL contexts and contradictions"""
        try:
            # CORE SEMANTIC UNDERSTANDING
            # The ML model automatically understands:
            # - "Vegetarian buffet" vs "ground chicken" = semantic contradiction
            # - "College friends trip" vs "historical academic" = context mismatch
            # - "Gluten-free" vs "wheat flour" = dietary conflict
            # - ANY context through pure semantic similarity
            
            job_embedding = self.model.encode([f"{persona} {job_description}"])
            content_embedding = self.model.encode([page_content])
            
            # BASE SEMANTIC SIMILARITY - This automatically understands everything!
            base_similarity = cosine_similarity(job_embedding, content_embedding)[0][0]
            
            # Detailed query-based analysis
            query_embeddings = self.model.encode(queries)
            chunks = self.smart_chunk_content(page_content)
            chunk_embeddings = self.model.encode(chunks)
            
            # Calculate detailed semantic relevance
            detailed_scores = []
            for i, query_emb in enumerate(query_embeddings):
                chunk_similarities = cosine_similarity([query_emb], chunk_embeddings)[0]
                
                # Weight primary query higher
                weight = 2.0 if i == 0 else 1.0
                
                avg_sim = np.mean(chunk_similarities)
                max_sim = np.max(chunk_similarities)
                
                query_score = (avg_sim * 0.6 + max_sim * 0.4) * weight
                detailed_scores.append(query_score)
            
            detailed_relevance = np.mean(detailed_scores)
            
            # PURE ML COMBINATION
            # The semantic similarity automatically handles:
            # - Dietary restrictions (vegetarian vs meat)
            # - Age appropriateness (college friends vs academic content)
            # - Context matching (buffet vs individual meals)
            # - ANY semantic relationship through learned embeddings
            final_relevance = (
                base_similarity * 0.4 +      # Overall semantic job-content match
                detailed_relevance * 0.6     # Detailed query-based matching
            )
            
            return final_relevance * 100
            
        except Exception as e:
            print(f"Error in pure semantic relevance: {e}")
            return 0
    
    def smart_chunk_content(self, text, max_chunk_size=300):
        """Smart content chunking using sentence boundaries"""
        try:
            sentences = sent_tokenize(text)
            chunks = []
            current_chunk = ""
            
            for sentence in sentences:
                if len(current_chunk + sentence) <= max_chunk_size:
                    current_chunk += " " + sentence
                else:
                    if current_chunk.strip():
                        chunks.append(current_chunk.strip())
                    current_chunk = sentence
            
            if current_chunk.strip():
                chunks.append(current_chunk.strip())
            
            return [chunk for chunk in chunks if len(chunk) > 80][:8]
            
        except Exception as e:
            return [text[i:i+max_chunk_size] for i in range(0, len(text), max_chunk_size)][:8]
    
    def create_refined_text(self, chunks, queries):
        """Create refined text using ML relevance ranking"""
        try:
            query_embeddings = self.model.encode(queries[:3])  # Top 3 queries
            chunk_embeddings = self.model.encode(chunks)
            
            chunk_scores = []
            for chunk_emb in chunk_embeddings:
                similarities = [
                    cosine_similarity([query_emb], [chunk_emb])[0][0] 
                    for query_emb in query_embeddings
                ]
                # Weight first query higher
                weighted_score = similarities[0] * 0.5 + np.mean(similarities[1:]) * 0.5 if len(similarities) > 1 else similarities[0]
                chunk_scores.append(weighted_score)
            
            # Sort by ML relevance
            chunk_score_pairs = list(zip(chunks, chunk_scores))
            chunk_score_pairs.sort(key=lambda x: x[1], reverse=True)
            
            # Build refined text
            refined_parts = []
            total_length = 0
            max_length = 500
            
            for chunk, score in chunk_score_pairs[:4]:
                if total_length + len(chunk) + 2 <= max_length:
                    refined_parts.append(chunk)
                    total_length += len(chunk) + 2
                else:
                    remaining = max_length - total_length
                    if remaining > 80:
                        refined_parts.append(chunk[:remaining-3] + "...")
                    break
            
            return " ".join(refined_parts)
            
        except Exception as e:
            return " ".join(chunks[:3])[:500]
    
    def pure_ml_analyze_document_content(self, document_name, content, persona, job_to_be_done):
        """Pure ML intelligence - automatically understands everything"""
        extracted_sections = []
        sub_section_analysis = []
        
        # Simple string conversion
        persona_str = str(persona) if not isinstance(persona, str) else persona
        job_str = str(job_to_be_done) if not isinstance(job_to_be_done, str) else job_to_be_done
        
        # Handle dict inputs
        if isinstance(persona, dict):
            persona_str = persona.get('role', '') or list(persona.values())[0] if persona else ""
        if isinstance(job_to_be_done, dict):
            job_str = job_to_be_done.get('task', '') or list(job_to_be_done.values())[0] if job_to_be_done else ""
        
        # Generate queries using ML
        queries = self.generate_ml_queries(persona_str, job_str)
        print(f"  üß† Generated {len(queries)} ML queries")
        print(f"  üéØ Primary query: {queries[0][:60]}...")
        
        page_count = 0
        for page_num, page_data in content["pages"].items():
            page_count += 1
            if page_count > 40:  # Process more pages
                break
            
            page_content = page_data.get('full_text', '')
            if not page_content or len(page_content) < 50:  # Lower threshold
                continue
            
            # PURE ML RELEVANCE - automatically understands ALL contexts
            relevance_score = self.calculate_pure_semantic_relevance(
                page_content, queries, persona_str, job_str
            )
            
            print(f"    üìÑ Page {page_num}: pure ML relevance {relevance_score:.1f}")
            
            # Lowered threshold to find more results
            if relevance_score > 8:  # Lower threshold
                section_title = self.extract_best_heading_ml(page_data)
                
                extracted_sections.append({
                    "document": document_name,
                    "section_title": section_title,
                    "importance_score": relevance_score,
                    "page_number": page_num
                })
                
                # Create refined text
                chunks = self.smart_chunk_content(page_content)
                refined_text = self.create_refined_text(chunks, queries)
                
                sub_section_analysis.append({
                    "document": document_name,
                    "refined_text": refined_text,
                    "page_number": page_num
                })
        
        # Sort by pure ML relevance
        extracted_sections.sort(key=lambda x: x["importance_score"], reverse=True)
        
        for i, section in enumerate(extracted_sections):
            section["importance_rank"] = i + 1
            del section["importance_score"]
        
        return extracted_sections, sub_section_analysis

# --- Utility functions ---
def read_input_json(input_path):
    """Read input JSON file"""
    try:
        with open(input_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
            persona = data.get("persona", "")
            job_to_be_done = data.get("job_to_be_done", "")
            
            if isinstance(persona, dict):
                persona = persona.get('role', '') or list(persona.values())[0] if persona else ""
            
            if isinstance(job_to_be_done, dict):
                job_to_be_done = job_to_be_done.get('task', '') or list(job_to_be_done.values())[0] if job_to_be_done else ""
            
            return persona, job_to_be_done
    except Exception as e:
        print(f"‚ùå Error reading input file: {e}")
        return None, None

def process_collection(collection_path, collection_name, analyzer):
    """Process collection with pure ML intelligence"""
    print(f"\n{'='*70}")
    print(f"üß† Processing {collection_name} (Pure ML Intelligence - Zero Hardcoding)")
    print(f"{'='*70}")
    
    collection_start_time = time.time()
    
    input_json_path = os.path.join(collection_path, "input.json")
    if not os.path.exists(input_json_path):
        input_files = [f for f in os.listdir(collection_path) 
                      if f.lower().startswith("input") and f.endswith(".json")]
        if input_files:
            input_json_path = os.path.join(collection_path, input_files[0])
        else:
            print(f"‚ùå No input.json found")
            return False
    
    persona, job_to_be_done = read_input_json(input_json_path)
    if not persona or not job_to_be_done:
        print(f"‚ùå Failed to read persona and job")
        return False
    
    print(f"üë§ Persona: {persona}")
    print(f"üéØ Job: {job_to_be_done}")
    
    pdfs_folder = os.path.join(collection_path, "PDFs")
    if not os.path.exists(pdfs_folder):
        for folder_name in ["pdfs", "PDF", "documents", "Documents"]:
            alt_folder = os.path.join(collection_path, folder_name)
            if os.path.exists(alt_folder):
                pdfs_folder = alt_folder
                break
        else:
            print(f"‚ùå PDFs folder not found")
            return False
    
    pdf_files = [f for f in os.listdir(pdfs_folder) if f.lower().endswith(".pdf")][:5]
    if not pdf_files:
        print(f"‚ùå No PDF files found")
        return False
    
    print(f"üìö Processing {len(pdf_files)} PDF files (Pure ML Intelligence)")
    
    all_extracted_sections = []
    all_sub_section_analysis = []
    input_doc_names = []
    
    for filename in pdf_files:
        pdf_path = os.path.join(pdfs_folder, filename)
        print(f"üìñ Pure ML Analysis: {filename}")
        
        pdf_start_time = time.time()
        content = analyzer.extract_text_with_formatting(pdf_path)
        
        if content["pages"]:
            input_doc_names.append(filename)
            
            sections, sub_sections = analyzer.pure_ml_analyze_document_content(
                filename, content, persona, job_to_be_done
            )
            all_extracted_sections.extend(sections)
            all_sub_section_analysis.extend(sub_sections)
            
            pdf_time = time.time() - pdf_start_time
            print(f"  ‚úÖ Analyzed in {pdf_time:.2f}s | Found {len(sections)} sections")
        else:
            print(f"  ‚ö†Ô∏è  Skipping {filename} (no extractable text)")
    
    if not all_extracted_sections:
        print(f"‚ùå No relevant content found")
        return False
    
    # Get top 5 sections
    all_extracted_sections.sort(key=lambda x: x["importance_rank"])
    top_5_sections = all_extracted_sections[:5]
    
    for i, section in enumerate(top_5_sections):
        section["importance_rank"] = i + 1
    
    top_5_keys = {(s["document"], s["page_number"]) for s in top_5_sections}
    top_5_subsections = [s for s in all_sub_section_analysis 
                        if (s["document"], s["page_number"]) in top_5_keys]
    
    print(f"\nüèÜ Pure ML Intelligence Top 5 Sections:")
    for section in top_5_sections:
        print(f"  {section['importance_rank']}. {section['section_title']} ({section['document']})")
    
    output_json = {
        "metadata": {
            "input_documents": input_doc_names,
            "persona": persona,
            "job_to_be_done": job_to_be_done,
            "processing_timestamp": datetime.datetime.now().isoformat()
        },
        "extracted_sections": top_5_sections,
        "subsection_analysis": top_5_subsections
    }
    
    output_path = os.path.join(collection_path, "output.json")
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_json, f, indent=2, ensure_ascii=False)
        
        collection_time = time.time() - collection_start_time
        print(f"‚úÖ Saved pure ML analysis to: {output_path}")
        print(f"‚è±Ô∏è  Collection processed in {collection_time:.2f} seconds")
        return True
    except Exception as e:
        print(f"‚ùå Error saving output: {e}")
        return False

def process_all_collections():
    """Process all collections with pure ML intelligence"""
    print("üöÄ Adobe India Hackathon: Challenge 1b - PURE ML INTELLIGENCE")
    print("üß† Zero Hardcoding | Automatic Context Understanding | Universal")
    print("="*75)
    
    total_start_time = time.time()
    
    try:
        analyzer = PureMLAnalyzer()
    except Exception as e:
        print(f"‚ùå Failed to initialize pure ML system: {e}")
        return
    
    collection_folders = []
    for item in os.listdir(COLLECTIONS_BASE_PATH):
        item_path = os.path.join(COLLECTIONS_BASE_PATH, item)
        if os.path.isdir(item_path) and item.lower().startswith("collection"):
            collection_folders.append((item_path, item))
    
    if not collection_folders:
        print("‚ùå No Collection folders found!")
        return
    
    collection_folders.sort()
    print(f"üìÅ Found {len(collection_folders)} collection(s)")
    
    successful_collections = 0
    
    for collection_path, collection_name in collection_folders:
        try:
            if process_collection(collection_path, collection_name, analyzer):
                successful_collections += 1
        except Exception as e:
            print(f"‚ùå Error processing {collection_name}: {e}")
    
    total_time = time.time() - total_start_time
    
    print(f"\n{'='*75}")
    print(f"üéâ PURE ML INTELLIGENCE PROCESSING COMPLETE")
    print(f"{'='*75}")
    print(f"‚úÖ Successfully processed: {successful_collections}/{len(collection_folders)} collections")
    print(f"‚è±Ô∏è  Total time: {total_time:.2f} seconds")
    print(f"üß† Pure ML: Automatic context understanding through semantic similarity")
    print(f"‚ùå ZERO Hardcoding: No separate functions for different scenarios")
    
    if successful_collections > 0:
        print(f"\nüí° Pure ML intelligence should automatically understand:")
        print(f"  ‚Ä¢ Vegetarian vs Meat content through semantic similarity")
        print(f"  ‚Ä¢ College friends vs Academic content through context matching")
        print(f"  ‚Ä¢ ANY context through learned embeddings!")

if __name__ == "__main__":
    process_all_collections()
